# https://www.youtube.com/watch?v=0GrciaGYzV0

'''
filosofie: zo snel mogelijk een oplossing maken, hoe slecht ook, je hebt dan een benchmark die je kunt gaan verbeteren
zeer verhelderende en praktische uitleg en script dat ik hieronder heb gebruikt en ook voor mijn commandline python (zo goed mogelijk) werkend heb gemaakt

punten om nog op voort te borduren:
- wat kan ik nog varieren en winnen met random_state en oob-score?
- hoe zit het met cross validation/test score?
- begrip van de accuracy maat, inzicht in true positives, false negatives, true negatives, false positives
- betere score nog mogelijk met Name/Ticket/PassengerId?
- waarom werkt de tweede versie van 'importance of the features' niet?
- kan ik nog iets met de importance? hoe is die samengesteld?
- is het gegenereerde model ook in te zien?
- hoe pas ik het model toe op een test set?
- volgende stap: feature engineering
'''


print("\n from sklearn.ensemble import RandomForestRegressor")
from sklearn.ensemble import RandomForestRegressor

print("\n \n from sklearn.metrics import roc_auc_score")
# The error metric. In this case c-stat (aka ROC/AUC) = accuracy
from sklearn.metrics import roc_auc_score

print("\n \n import pandas as pd")
# An efficient data structure
import pandas as pd

print("\n \n import matplotlib.pyplot as plt")
import matplotlib.pyplot as plt

print("\n Import the data")
# Import the data
print("\n X = pd.read_csv...")
X = pd.read_csv("data/train.csv",delimiter=';')
print("\n y = X.pop(Survived)")
y = X.pop("Survived")
# 890, Name: Survived, dtype: int64, waarden: 0,1

print("\n X.describe(), ik mis de numerieke waarden age en fare. ze zijn als object opgenomen en niet als float64 zoals de andere numerieke waarden, misschien omdat er missing values zijn")
print(X.describe())

#print("\n beschrijven van alle waarden in de trainingsset, dus niet alleen de numerieke")
#print([X[column].describe() for column in list(X)])

#dat ga ik corrigeren voor de kolom "Age", ik maak er een string van, vervang alle komma's door punten
print("\n kolom Age wordt float gemaakt, de missende waarden worden vervangen door het gemiddelde") 
X["Age"] = [str(item) for item in X["Age"]]
X["Age"] = [item.replace(",",".") for item in X["Age"]]
#ik maak een extra float kolom Lftd aan om het gemiddelde te kunnen berekenen
X["Lftd"] = [item.replace(",",".") for item in X["Age"]]
X["Lftd"] = [float(item) for item in X["Lftd"]]
meanlftd = str(X["Lftd"].mean())
#de nan-waarden vervang ik door het gemiddelde
X["Age"] = [item.replace("nan",meanlftd) for item in X["Age"]]
#ik converteer "Age" naar het float-type en drop de Lftd-kolom
X["Age"] = [float(item) for item in X["Age"]]
X = X.drop("Lftd", 1)

#dat ga ik corrigeren voor de kolom "Fare", op dezelfde manier, ik maak er een string van, vervang alle komma's door punten
print("\n kolom Fare wordt float gemaakt, de missende waarden worden vervangen door het gemiddelde") 
X["Fare"] = [str(item) for item in X["Fare"]]
X["Fare"] = [item.replace(",",".") for item in X["Fare"]]
#ik maak een extra float kolom Tarief aan om het gemiddelde te kunnen berekenen
X["Tarief"] = [item.replace(",",".") for item in X["Fare"]]
X["Tarief"] = [float(item) for item in X["Tarief"]]
meanTarief = str(X["Tarief"].mean())
#de nan-waarden vervang ik door het gemiddelde
X["Fare"] = [item.replace("nan",meanTarief) for item in X["Fare"]]
#ik converteer "Fare" naar het float-type en drop de Tarief-kolom
X["Fare"] = [float(item) for item in X["Fare"]]
X = X.drop("Tarief", 1)

print("\n X.describe(), nu zijn ook de waarden age en fare numeriek, geen missing values meer")
print(X.describe())

# Get just the numeric variables by selecting only the variables that are not "object" datatypes.
print("\n Get just the numeric variables by selecting only the variables that are not object datatypes.")
numeric_variables = list(X.dtypes[X.dtypes != "object"].index)
print("\n de head van X[numeric_variables]")
print(X[numeric_variables].head())

# Het lijkt gek om de passengerid erin te houden, maar als het inderdaad waardeloze info is, zal random forest het toch negeren.

# Fit the model
print("\n model = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=42)")
model = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=42)
print("\n model.fit(X[numeric_variables], y)")
model.fit(X[numeric_variables], y)
print("\n model.oob_score")
model.oob_score
print("\n y_oob = model.oob_prediction_")
y_oob = model.oob_prediction_
print("\n c-stat: roc_auc_score(y, y_oob) zo goed is het model nu:")
print "c-stat: ", roc_auc_score(y, y_oob)

#print("\n y_oob (out of bag, de predictions of survival)")
#print y_oob

# Here is a simple function to show descriptive stats on the categorical variables
print("\n definitie van describe_categorical(X))")
def describe_categorical(X):
	print X[X.columns[X.dtypes == "object"]].describe()

print("\n describe_categorical(X)")
describe_categorical(X)

# We hebben geen zin in Name, Ticket en PassengerId, dus die droppen we.
print("\n drop columns Name, Ticket en PassengerId")
X.drop(["Name", "Ticket", "PassengerId"], axis=1, inplace = True)

# Change the Cabin variable to be only the first letter or None
print("\n definitie van functie clean_cabin: eerste letter of None")
def clean_cabin(x):
	try:
		return x[0]
	except TypeError:
		return "None"

# Look at all the columns in the dataset
#print("\n X.head()")
#print(X.head())

print("\n de functie clean_cabin toepassen op de kolom Cabin")
X["Cabin"] = X.Cabin.apply(clean_cabin)

# Look at all the columns in the dataset
#print("\n X.head()")
#print(X.head())

print("\n categorical_variables = ['Sex', 'Cabin', 'Embarked']")
categorical_variables = ['Sex', 'Cabin', 'Embarked']

print("\n De categorical kolommen slim vervangen door kolommen met het woord Missing bij missing variables en van alle mogelijke waarden een eigen True/False column maken (ik snap niet hoe)")
for variable in categorical_variables:
	# Fill missing data with the word "Missing"
	X[variable].fillna("Missing", inplace=True)
	# Create array of dummies
	dummies = pd.get_dummies(X[variable], prefix=variable)
	# Update X to include dummies and drop the main variable
	X = pd.concat([X, dummies], axis = 1)
	X.drop([variable], axis = 1, inplace = True)

# Look at all the columns in the dataset
print("\n X.head()")
print(X.head())

# Fit the model again
print("\n model = RandomForestRegressor(n_estimators=100, oob_score=True, n_jobs=-1, random_state=42)")
model = RandomForestRegressor(n_estimators=100, oob_score=True, n_jobs=-1, random_state=42)
print("\n model.fit(X, y)")
model.fit(X, y)
print("\n c-stat: roc_auc_score(y, model.oob_prediction_) zo goed is het model nu (beter dan net):")
print "c-stat: ", roc_auc_score(y, model.oob_prediction_)

# Variable importance measures (cool: welke features zijn belangrijk in het model)
print("\n model.feature_importances_")
print(model.feature_importances_)

# Dat was niet makkelijk te lezen, scientific notation en je moet opzoeken welke feature dan op plaats nummer zoveel stond. Maar we gaan het mooi maken.
print("\n horizontale staafdiagram: importance of the features")
feature_importances = pd.Series(model.feature_importances_, index=X.columns)
feature_importances.sort(ascending=True)
feature_importances.plot(kind = "barh", figsize=(7,6))

plt.title("Importance of the features (versie 1)")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.show()

# Nu willen we de bij elkaar horende features aggregeren om een nog beter beeld te krijgen (functie wordt niet behandeld in het filmpje)
print("\n definitie van functie graph_feature_importances")
def graph_feature_importances(model, feature_names, autoscale=True, headroom=0.05, width=10, summarized_columns=None):
	
	if autoscale:
		x_scale = model.feature_importances_.max()+ headroom
	else:
		x_scale = 1

	feature_dict=dict(zip(feature_names, model.feature_importances_))

	if summarized_columns:
		# some dummy columns need to be summarized
		for col_name in summarized_columns:
			# sum all the features that contain col_name, store in temp sum_value
			sum_value = sum(x for i, x in feature_dict.iteritems() if col_name in i )

		# now remove all keys that are part of col_name
		keys_to_remove = [i for i in feature_dict.keys() if col_name in i ]
		for i in keys_to_remove:
			feature_dict.pop(i)
		# lastly, read the summarized field
		feature_dict[col_name] = sum_value

	results = pd.Series(feature_dict.values(), index=feature_dict.keys())
	results.sort(axis=1)
	results.plot(kind="barh", figsize=(width,len(results)/4), xlim=(0,x_scale))
	# print(results) # hier zie je ook dat het niet gelukt is, dus het is geen grafisch probleem.

graph_feature_importances(model, X.columns, summarized_columns=categorical_variables)

print("\n Dit zou een horizontale staafdiagram van de geaggregeerde features moeten zijn, maar dat is het niet.")
print("\n Sex is de belangrijkste, daarna respectievelijk Age, Fare, Pclass, Cabin, SbSp, Embarked en als laatste Parch")

plt.title("Importance of the features (versie 2)")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.show()

print("\n Parameter tests (behalve random_state en oob_score want die hadden we al gehad)")
print("\n Parameters that will make your model better:")
print(" - n_estimators: The number of trees in the forest. Choose as high of a number as your computer can handle.")
print(" - max_features: The number of features to consider when looking for the best slplit. Try[auto, None, sqrt, log2, 0.9, and 0.2]")
print(" - min_samples_leaf: The minimum number of samples in newly created leaves. Try [1, 2, 3]. If 3 is the best, try higher numbers such as 1 through 10.")
print("\n Parameters that will make it easier to train your model:")
print(" - n_jobs: Determines if multiple processors should be used to train and test the model. Always set this to -1 and %%timeit vs. if it is set to 1. It should be much faster (especially when 

many trees are trained).")

print("\n n_jobs (helaas, de %%timeit werkt niet, het is een ipython notebook functie die de code in een ipython notebook cel timet)")
print("n_jobs = -1 blijkt iets sneller te zijn dan n_jobs = 1 (1.3 s per loop vs. 2.01 s per loop)")
'''
%%timeit
model = RandomForestRegressor(1000, oob_score=True, n_jobs=1, random_state=42)
model.fit(X, y)

%%timeit
model = RandomForestRegressor(1000, oob_score=True, n_jobs=-1, random_state=42)
model.fit(X, y)
'''

print("\n n_estimators: varieren voor een verschillend aantal trees, kijken welke de beste c-stat-waarde oplevert.")
results = []
print("\n n_estimator_options = [30, 50, 100, 200, 500, 1000, 2000] \n")
n_estimator_options = [30, 50, 100, 200, 500, 1000, 2000]

for trees in n_estimator_options:
	model = RandomForestRegressor(trees, oob_score=True, n_jobs=-1, random_state=42)
	model.fit(X, y)
	print trees, "trees"
	roc = roc_auc_score(y, model.oob_prediction_)
	print "C-stat: ", roc
	results.append(roc)
	print ""

pd.Series(results, n_estimator_options).plot()
print("\n Bij 100 trees doet ie het het beste, maar het scheelt niet veel, zie lijndiagram. We kiezen 1000 in het vervolg.")
plt.show()

print("\n max_features")
results = []
max_features_options = ["auto", None, "sqrt", "log2", 0.9, 0.2]
print("\n max_features gaat over hoeveel van je variabelen je gebruikt. sqrt is de wortel van je aantal variabelen, 0.9 is 90% van je variabelen. auto en none gebruiken (hier) alle variabelen. \n")

for max_features in max_features_options:
	model = RandomForestRegressor(n_estimators=1000, oob_score=True, n_jobs=-1, random_state=42, max_features=max_features)
	model.fit(X, y)
	print max_features, "option"
	roc = roc_auc_score(y, model.oob_prediction_)
	print "C-stat: ", roc
	results.append(roc)
	print ""

pd.Series(results, max_features_options).plot(kind="barh", xlim=(.85,.88))
print("\n Auto en None zijn het beste. We gaan met de auto.")
plt.show()

print("\n min_samples_leaf \n")
results = []
min_samples_leaf_options = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

for min_samples in min_samples_leaf_options:
	model = RandomForestRegressor(n_estimators=1000, oob_score=True, n_jobs=-1, random_state=42, max_features="auto", min_samples_leaf=min_samples)
	model.fit(X, y)
	print min_samples, "min samples"
	roc = roc_auc_score(y, model.oob_prediction_)
	print "C-stat: ", roc
	results.append(roc)
	print ""


pd.Series(results, min_samples_leaf_options).plot()
print("\n 5 is het beste")
plt.show()

print("\n Final model")
model = RandomForestRegressor(n_estimators=1000, oob_score=True, n_jobs=-1, random_state=42, max_features="auto", min_samples_leaf=5)
model.fit(X, y)
roc = roc_auc_score(y, model.oob_prediction_)
print "C-stat: ", roc
